{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21761bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from mxifpublic.community.graph_utils import (count_mask_pixels_in_radius,\n",
    "                                              threshold_graph_edges_by_distance,\n",
    "                                              generate_graph_adj_matrix,\n",
    "                                              calculate_edge_length_statistic)\n",
    "from mxifpublic.plotting.plot import cell_typing_plot\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import ARGVA, GCNConv\n",
    "from tqdm.notebook import tqdm\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7402e",
   "metadata": {},
   "source": [
    "# Constants specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af6c8d-ad43-444d-8cf9-fc844d4370b1",
   "metadata": {},
   "source": [
    "In order to conduct community analysis, one must have cell-type-to-segment annotations in the following format (denoted below as cell_type_segment_annotation.csv):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d64951-3e46-469f-a384-0c5f5edc7e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T13:31:53.191811Z",
     "iopub.status.busy": "2024-09-11T13:31:53.191164Z",
     "iopub.status.idle": "2024-09-11T13:31:53.205576Z",
     "shell.execute_reply": "2024-09-11T13:31:53.204079Z",
     "shell.execute_reply.started": "2024-09-11T13:31:53.191759Z"
    }
   },
   "source": [
    " index | sample_id | center_x | center_y | cell_type \n",
    " --- | --- | --- | --- | --- \n",
    " 1 | HP9849 | 1200 | 2300 | B-cells \n",
    " 2 | HP9849 | 1212 | 2567 | B-cells \n",
    " 3 | HP9849 | 1232 | 3411 | T-helpers \n",
    " ... | ... | ... | ... | ...\n",
    " 3450022 | HP9976 | 3476 | 1012 | Macrophages \n",
    " 3450023 | HP9976 | 3490 | 2489 | B-cells \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a849f-0f4b-4a6f-a5b7-c79cb6ebd722",
   "metadata": {},
   "source": [
    "Raw image data in Imaris (.ims) format can be found in the IDR repository (https://idr.openmicroscopy.org) under accession number idr0158. To use the code provided below, first convert the .ims images to TIFF format, ensuring that each channel is saved as a separate TIFF image. Place these images in a designated folder, such as IBEX_raw_images. Ensure that the IBEX_raw_images directory contains the raw images with the following naming convention: {sample_id}_{marker}.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfd10aef",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "samples = ['HP9849', 'HP9976']\n",
    "\n",
    "path_cells = 'cell_type_segment_annotation.csv'\n",
    "path_masks = 'IBEX_raw_images/'\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "markers = [\n",
    " 'CD21',\n",
    " 'CD31',\n",
    " 'CD68',\n",
    "# Specify all markers of interest  \n",
    "# ...\n",
    "}\n",
    "    \n",
    "radius = 100\n",
    "node_fetures_to_add = ['center_x', 'center_y', 'cell_type', 'index']\n",
    "\n",
    "distance_threshold = 200\n",
    "center_x_col_name, center_y_col_name = 'center_x', 'center_y'\n",
    "\n",
    "features_columns = [\n",
    "    f'nb_num_pixels_{radius}_{marker}' for marker in markers\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607c606",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ebd6d",
   "metadata": {},
   "source": [
    "## Mask percentages calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08768d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = pd.read_csv(path_cells)\n",
    "\n",
    "# Specify here other samples if needed\n",
    "cells = cells[cells['sample_id'].isin(samples)].reset_index(drop=True)\n",
    "rois = []\n",
    "\n",
    "for sample_id, sample_data in tqdm(cells.groupby('sample_id')):\n",
    "    for marker in tqdm(markers, leave=False):\n",
    "        image = imread(f'{path_masks}/{sample_id}_{marker}.tif')\n",
    "\n",
    "        # It is recommended to verify whether the Otsu method provides an appropriate threshold value; if it does not, manually set the threshold.\n",
    "        mask = (image > threshold_otsu(image)).astype('uint8') \n",
    "        \n",
    "        neighbour_pixels = count_mask_pixels_in_radius(\n",
    "            mask, sample_data[['center_x', 'center_y']].values, radius=radius\n",
    "        )\n",
    "\n",
    "        feature_name = f'nb_num_pixels_{radius}_{mask_name}'\n",
    "        sample_data[feature_name] = neighbour_pixels\n",
    "        node_fetures_to_add.append(feature_name)\n",
    "    rois.append(sample_data)\n",
    "    \n",
    "cells = pd.concat(rois).reset_index(drop=True)\n",
    "cells['cell_index'] = cells['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f897f",
   "metadata": {},
   "source": [
    "## Assemble graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_graphs = dict()\n",
    "cell_distances = []\n",
    "\n",
    "for sample_id, roi_data in tqdm(cells.groupby('sample_id')):\n",
    "    adjacency_matrix = generate_graph_adj_matrix(roi_data)\n",
    "    adjacency_matrix = threshold_graph_edges_by_distance(adjacency_matrix, distance_threshold)\n",
    "    median_edge_distance = calculate_edge_length_statistic(adjacency_matrix, distance_threshold)\n",
    "    \n",
    "    cell_distances.append(median_edge_distance)\n",
    "    patient_graphs[sample_id] = (adjacency_matrix, median_edge_distance)\n",
    "\n",
    "cell_distances = np.concatenate(cell_distances)\n",
    "\n",
    "cell_distance_scaler = MinMaxScaler()\n",
    "cell_distance_scaler.fit(cell_distances.reshape(-1, 1))\n",
    "\n",
    "patient_graphs = {\n",
    "    key: (adj, cell_distance_scaler.transform(dst.reshape(-1, 1)))\n",
    "    for key, (adj, dst) in patient_graphs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8ef1f",
   "metadata": {},
   "source": [
    "# Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_encoder = OneHotEncoder(sparse=False)\n",
    "cell_type_encoder.fit(cells['cell_type'].values.reshape(-1, 1))\n",
    "\n",
    "cohort_graph_dataset = []\n",
    "\n",
    "for sample_id, roi_data in tqdm(cells.groupby('sample_id')):\n",
    "    adjacency_matrix, median_edge_distance = patient_graphs[sample_id]\n",
    "    edge_indices, _ = from_scipy_sparse_matrix(adjacency_matrix)\n",
    "    cell_types = cell_type_encoder.transform(roi_data['cell_type'].values.reshape(-1, 1))\n",
    "    \n",
    "    mask_percentages = roi_data[features_columns]\n",
    "\n",
    "    node_data = np.concatenate([cell_types, median_edge_distance, mask_percentages], axis=1)\n",
    "\n",
    "    sample_data_object = Data(edge_index=edge_indices,\n",
    "                              x=torch.Tensor(node_data).float(),\n",
    "                              contour_index=cells['cell_index'],\n",
    "                              sample_id=sample_id)\n",
    "\n",
    "    cohort_graph_dataset.append(sample_data_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad54b4",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca8ba1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "467af9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# +1 is required for median distance\n",
    "in_channels = len(cells['cell_type'].unique()) + len(mask_paths) + 1\n",
    "model_encoder_configuration = [in_channels, 32, 32]\n",
    "model_decoder_configuration = [32, 64, 32]\n",
    "device = 'cpu'\n",
    "num_epochs = 10\n",
    "num_repeat_discriminator = 5\n",
    "discriminator_lr = 0.001\n",
    "encoder_lr = 0.005\n",
    "save_models_dir = 'models'\n",
    "features_num = model_encoder_configuration[2]\n",
    "n_communities = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816508b7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a3f28b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels, cached=False)\n",
    "        self.conv_mu = GCNConv(hidden_channels, out_channels, cached=False)\n",
    "        self.conv_logstd = GCNConv(hidden_channels, out_channels, cached=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = torch.nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.lin3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "435aeaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(*model_encoder_configuration)\n",
    "discriminator = Discriminator(*model_decoder_configuration)\n",
    "model = ARGVA(encoder, discriminator)\n",
    "\n",
    "device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "model.train()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7115e99e",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7917dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(save_models_dir, exist_ok=True)\n",
    "discriminator_optimizer = torch.optim.Adam(model.discriminator.parameters(), lr=discriminator_lr)\n",
    "encoder_optimizer = torch.optim.Adam(model.encoder.parameters(), lr=encoder_lr)\n",
    "best_loss = 10**9\n",
    "\n",
    "dataloader = DataLoader(cohort_graph_dataset, batch_size=1, shuffle=True)\n",
    "loss_history = []\n",
    "for epoch in tqdm(range(num_epochs)): \n",
    "    mean_epoch_loss = []\n",
    "    for data_train in dataloader:\n",
    "        encoder_optimizer.zero_grad()\n",
    "        data_train = data_train.to(device)\n",
    "        \n",
    "        z = model.encode(data_train.x, data_train.edge_index)\n",
    "\n",
    "        for i in range(num_repeat_discriminator):\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            discriminator_loss = model.discriminator_loss(z)\n",
    "            discriminator_loss.backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "        loss = model.recon_loss(z, data_train.edge_index)\n",
    "        loss = loss + model.reg_loss(z)\n",
    "        loss = loss + (1 / data_train.num_nodes) * model.kl_loss()\n",
    "        \n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        \n",
    "        mean_epoch_loss.append(loss.item())\n",
    "    \n",
    "    mean_epoch_loss = np.mean(mean_epoch_loss)\n",
    "    loss_history.append(mean_epoch_loss)\n",
    "    print(f'Epoch: {epoch} Mean epoch loss: {mean_epoch_loss}')\n",
    "    \n",
    "    if best_loss > mean_epoch_loss:\n",
    "        best_loss = mean_epoch_loss\n",
    "        torch.save(model.state_dict(), f'{save_models_dir}/community_model_{mean_epoch_loss:.4f}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953255e",
   "metadata": {},
   "source": [
    "## Loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "plt.plot(loss_history, color='black')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss history')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b482eefa",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b4f2429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c30f1c191c439eaffa560b3fec6dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(f'{save_models_dir}/community_model_{best_loss:.4f}.pth',\n",
    "                                 map_location=device))\n",
    "cohort_embeddings = []\n",
    "for sample_data_object in tqdm(cohort_graph_dataset):\n",
    "    sample_id = sample_data_object.sample_id\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        x, edge_index = (sample_data_object.x.to(device), \n",
    "                         sample_data_object.edge_index.to(device))\n",
    "\n",
    "        z = model.encode(x, edge_index)\n",
    "        z = z.squeeze()\n",
    "        z = z.detach().cpu().numpy()\n",
    "\n",
    "    node_embeddings = pd.DataFrame(z)\n",
    "    node_embeddings['sample_id'] = sample_id\n",
    "    node_embeddings['cell_index'] = sample_data_object.contour_index\n",
    "    cohort_embeddings.append(node_embeddings)\n",
    "    \n",
    "cohort_embeddings = pd.concat(cohort_embeddings)\n",
    "cohort_embeddings = cohort_embeddings.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac826551",
   "metadata": {},
   "source": [
    "## Cluster embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "027edc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = MiniBatchKMeans(n_clusters=n_communities, random_state=random_state)\n",
    "embedding_clusters = clusterer.fit_predict(cohort_embeddings[list(range(features_num))].values)\n",
    "embedding_clusters = pd.Series(embedding_clusters, name='graph_cluster')\n",
    "embedding_clusters = 'cluster_' + embedding_clusters.apply(str)\n",
    "cohort_embeddings['graph_cluster'] = embedding_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7a2e4",
   "metadata": {},
   "source": [
    "## Relative content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e5906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    " '#11112e',\n",
    " '#0ca7ef',\n",
    " '#1b1bc0',\n",
    " '#041e72',\n",
    " '#5A00FF',\n",
    " '#CBD855',\n",
    " '#3BA31B',\n",
    " '#005222',\n",
    " '#1E82AF',\n",
    " '#00A38B',\n",
    " '#8BF16A',\n",
    " '#cc751f',\n",
    " '#f6bd60',\n",
    " '#5C493E',\n",
    " '#b196b3'\n",
    "]\n",
    "# Palette is generated only if there are less than 16 community clusters\n",
    "assert n_communities <= len(colors)\n",
    "colors = random.sample(colors, n_communities)\n",
    "palette = {cluster: color for cluster, color in zip([f'cluster_{i}' for i in range(n_communities)], \n",
    "                                                     colors)}\n",
    "pivot = pd.pivot_table(cohort_embeddings, \n",
    "                       index='sample_id',\n",
    "                       columns='graph_cluster',\n",
    "                       values='cell_index',\n",
    "                       aggfunc=len,\n",
    "                       fill_value=0)\n",
    "pivot = pivot.div(pivot.sum(axis=1), axis=0) * 100\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 8))\n",
    "pivot.plot.bar(stacked=True, legend=False, \n",
    "               ax=ax, color=palette)\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Community relative content')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clusterenv",
   "language": "python",
   "name": "clusterenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
